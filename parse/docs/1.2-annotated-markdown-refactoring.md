## Annotated Markdown test extraction — plain-English algorithm

This document describes, in plain English and step-by-step, how the test producer implemented in
`parse/tests/test-annotated.js` extracts annotated test cases from Markdown files and turns them into
`node:test` test cases. It follows the script's logic top-to-bottom and explains the data shapes and
intent behind each step.

1. Setup and imports
- Load standard Node modules (`fs`, `path`, `assert`, `test` and URL helpers).
- Import scanner utilities: `getTokenKind` and `getTokenLength` from `scan-core.js`, all token constants
	from `scan-tokens.js`, and the main `scan0` scanning function from `scan0.js`.
- Compute `__dirname` for the ESM module so tests can find files relative to the script.

2. Discover Markdown test files
- Recursively search the `parse/tests` directory for files with the `.md` extension.
- Collect a list of absolute paths to every `.md` file found; these files will be scanned for annotated blocks.

3. Break a file into annotated "blocks"
- Read the file and split into lines (normalize CRLF to LF first).
- Walk the lines looking for a marker line: a line composed only of spaces/tabs and alphanumeric characters
	(at least one alphanumeric character). Such a line is treated as the marker line that sits under content.
- When a marker line is found:
	- Collect the content lines immediately above it. The code backtracks from the marker and gathers
		previous lines until the cumulative length of collected lines is at least 10 characters or start-of-file
		is reached. This yields a small block of content lines assumed to be the annotated source.
	- Prepend a synthetic first line of content in the form `at <repoRelativePath>:<markerLineNumber>` to help
		with traceability.
	- Collect assertion lines: gather every line after the marker that starts with `@` — these are the
		annotation assertions for the marker positions.
	- Capture two lines of post-context that come after the `@` assertions (used later when producing diffs).
	- If both content and assertion lines were found, package these into a `block` object containing:
		startLine, content[], markerLine, assertions[], and after[] (two lines of post context).
- Continue scanning the file after the collected assertions, repeating for additional blocks.

4. Helpers for tokens and assertions
- decodeProvisionalToken(tok): given a provisional token integer, extract its `length` (lower 24 bits) and
	`flags` (the high bits) using the provided `getTokenLength` and `getTokenKind` helpers.
- parseAssertionLine(line): parse an `@` line into two pieces: an optional token type name (like `InlineText`)
	and an optional quoted expected text (parsed as JSON). The parser tolerates either `@1 Name "text"` or
	`@1 "text"` forms and handles escaped quotes.
- mapAssertions(markerLine, assertions): build a map keyed by annotation label (the token after `@`, e.g.
	`1` or `A`) to the parsed assertion data from `parseAssertionLine`.

5. For each annotated block, create a test case
- Construct a readable test name using the last content line, a normalized marker line, the file path and
	the starting line number.
- Register a `node:test` test with that name. The test body performs the scanning and verification.

6. Prepare scanning input
- Join every `content` line in the block using `\n` into a single `input` string. Note: the `content` array
	begins with the synthetic `at ...` line added earlier.

7. Scan the input fully using `scan0`
- Initialize an empty `output` array to receive provisional token integers and set `currentOffset = 0`.
- Repeatedly call `scan0({ input, startOffset: currentOffset, endOffset: input.length, output })`.
	- `scan0` appends provisional token integers to `output` and returns how many tokens it appended.
	- If `scan0` returns zero tokens, break the loop to avoid infinite loops.
	- Otherwise compute how many characters were consumed by summing the lengths of the newly appended tokens
		(using `getTokenLength`) and advance `currentOffset` by that sum.
- Continue until `currentOffset >= input.length` or `scan0` yields zero tokens.

8. Convert provisional tokens to token records
- Iterate the `output` provisional tokens in order; keep a running `acc` character offset starting at 0.
- For each provisional integer `raw`, call `decodeProvisionalToken(raw)` to extract `length` and `flags`.
- Push a token record { start: acc, end: acc + length, flags, raw } into an array, and increment `acc` by `length`.
- The resulting `tokens` array represents token spans measured against the `input` buffer that was scanned.

9. Align marker characters to positions and assertions
- Compute the character offset within `input` where the last content line begins (`lastLineOffset`) by summing
	lengths of prior content lines and newlines.
- Parse `markerLine` into `positionMarkerChars` and their column offsets: for each non-whitespace char in the
	marker line, record the character (label) and its column index.
- Create `positionMarkerSlots`: one slot per marker char holding label, the column offset, and placeholders for
	the token it points to, any assertion text, and an expected token name.
- For each collected `@` assertion line, extract the label after `@` and attach that `@` line (and the expected
	token name if present) to the corresponding slot by matching the label case-insensitively to marker chars.

10. Map slots to tokens and detect mismatches
- For each slot, compute an absolute position `absPos = lastLineOffset + slot.lineOffset` into `input`.
- Find the token in `tokens` that covers `absPos` (a token `t` such that `t.start <= absPos < t.end`).
	- If no such token exists, record the slot as missing and mark a mismatch.
	- If a covering token exists, store its start in the slot and mark a mismatch if the marker did not point
		exactly at the token start (i.e., `covering.start !== absPos`).
- Build a mapping from each unique token start to the corresponding slot index for canonical ordering.

11. Build canonical actual and expected report lines
- Collect a canonical ordered list of token starts (sorted ascending) from the mapping.
- Prepare `actualReportLines` and `expectedLines`:
	- If any mismatch or missing slot was detected, add the first one or two content lines as headers to both
		actual and expected (padding to two lines if needed). This creates a compact diagnostic header.
	- Otherwise, include all content lines verbatim in both actual and expected.
- For each ordered token start, emit a canonical marker label (1..9, then A,B,...) placed at the token's
	column within a `positionLine` string, looking up the actual token by start index.
- Decode the token's flags and produce a list of token name(s) that match the bitmask using `PARSE_TOKENS`.
- For each slot associated with the token:
	- If the original annotation had no expected token name, synthesize an assertion line `@<marker> <names|flags>`
		describing what was actually found.
	- If there was an expected token name, check whether the token flags include that flag. If so, append the
		original `@` line with its label rewritten to the canonical marker. If not, append a diagnostic canonical
		`@<marker> <names|flags>` and flag a mismatch.
- After processing tokens, right-pad the `positionLine` to the original marker line length, append it to
	`actualReportLines`, then append all constructed assertion lines. If there were mismatches, append the
	stored `after` (post-context) lines as well.

12. Build the expected block text
- The `expected` text combines the header or content lines, the original `markerLine`, and the original
	`@` assertion lines. If mismatches were detected, it also appends the two post-context lines.

13. Final equality check
- If mismatches or missing slots were detected, the test invokes `assert.strictEqual(actualReport, expected)`.
	This intentionally fails and shows a diff between the canonicalized actual diagnostics and the original
	annotated block, helping the developer see where tokenization or annotation expectations differ.
- If there were no mismatches, the test completes without explicit assertion failures.

14. Important notes and limitations observed in the implementation
- The script prepends a synthetic `at <file>:<line>` line to the content being scanned. This alters offsets
	inside the `input` string relative to the original file, which can cause misalignment between markers and
	true file positions unless carefully accounted for.
- When collecting content lines above a marker, the script only gathers enough lines to reach a minimal
	cumulative length (>= 10 characters). This heuristic can drop earlier context that affects tokenization and
	may produce incorrect test expectations.
- The algorithm expects simple marker lines made of whitespace and alphanumeric characters only and will skip
	other formats.
- The scanning loop advances by sums of produced token lengths; if `scan0` ever returns zero tokens the loop
	stops early but does not raise a test error. This may mask partial scans.

15. Summary
- The script finds annotated blocks in Markdown files by locating special marker lines, extracts a small chunk
	of content above them and `@` assertions below them, scans that constructed content with `scan0`, maps
	marker characters to token positions, and compares a canonicalized actual diagnostic output with the
	original annotated assertions. Failures result in a strict equality assertion which surfaces a diff for
	debugging.

If you want, I can now propose precise fixes to ensure the whole file is used (not a short chunk), preserve
absolute offsets (do not prepend synthetic lines), and make the diagnostics easier to interpret. I can also
implement and test those changes in `parse/tests/test-annotated.js`.